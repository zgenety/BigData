{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "baa28891-8f4d-48dd-9381-8c85c5a86565",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "team = 39\n",
    "\n",
    "warehouse = \"project/hive/warehouse\"\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "        .appName(\"{} - spark ML\".format(team))\\\n",
    "        .master(\"yarn\")\\\n",
    "        .config(\"hive.metastore.uris\", \"thrift://hadoop-02.uni.innopolis.ru:9883\")\\\n",
    "        .config(\"spark.sql.warehouse.dir\", warehouse)\\\n",
    "        .config(\"spark.sql.avro.compression.codec\", \"snappy\")\\\n",
    "        .enableHiveSupport()\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "17eab72c-f832-4554-a3ac-eba6f01b84e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = spark.read.format(\"avro\").table('team39_projectdb.metrics_part')\n",
    "stations = spark.read.format(\"avro\").table('team39_projectdb.stations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5ee13134-459a-435a-8126-fb7219da3eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = metrics.join(stations, on='sid', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a05eb1d6-3e0e-4bed-bae2-9e519cad63e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCols\n",
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "class Encoder(Transformer, HasInputCol, HasOutputCols, DefaultParamsReadable, DefaultParamsWritable):\n",
    "    def __init__(self, inputCol, outputCols, n):\n",
    "        super(Encoder, self).__init__()\n",
    "        self._set(inputCol=inputCol, outputCols=outputCols)\n",
    "        self.n = n\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        input_col = self.getInputCol()\n",
    "        output_cols = self.getOutputCols()\n",
    "        \n",
    "        dataset = dataset.withColumn(output_cols[0], F.sin(2 * math.pi * F.col(input_col) / self.n))\n",
    "        \n",
    "        return dataset\n",
    "    \n",
    "    \n",
    "df = df.withColumn(\"year\", F.year(\"date_time\"))\n",
    "df = df.withColumn(\"month\", F.month(\"date_time\"))\n",
    "df = df.withColumn(\"day\", F.dayofmonth(\"date_time\"))\n",
    "df = df.withColumn(\"hour\", F.hour(\"date_time\"))\n",
    "df = Encoder(\"month\", [\"month\"], 12).transform(df)\n",
    "df = Encoder(\"day\", [\"day\"], 31).transform(df)\n",
    "df = Encoder(\"hour\", [\"hour\"], 24).transform(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "162209e0-9a51-41ab-b9d2-f15a4e83b804",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasInputCols, HasOutputCols\n",
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType\n",
    "\n",
    "class GeodeticToECEFTransformer(Transformer, HasInputCols, HasOutputCols, DefaultParamsReadable, DefaultParamsWritable):\n",
    "    def __init__(self, inputCols=None, outputCols=None, defaultAltitude=0.0):\n",
    "        super(GeodeticToECEFTransformer, self).__init__()\n",
    "        self._set(inputCols=inputCols, outputCols=outputCols)\n",
    "        self.defaultAltitude = defaultAltitude\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        input_cols = self.getInputCols()\n",
    "        output_cols = self.getOutputCols()\n",
    "        \n",
    "        a = 6378137.0\n",
    "        f = 1 / 298.257223563\n",
    "        b = a * (1 - f)\n",
    "        e2 = 1 - (b ** 2) / (a ** 2)\n",
    "        \n",
    "        def geodetic_to_ecef(lat, lon, alt):\n",
    "            lat_rad = math.radians(lat)\n",
    "            lon_rad = math.radians(lon)\n",
    "            \n",
    "            N = a / math.sqrt(1 - e2 * math.sin(lat_rad) ** 2)\n",
    "            \n",
    "            x = (N + alt) * math.cos(lat_rad) * math.cos(lon_rad)\n",
    "            y = (N + alt) * math.cos(lat_rad) * math.sin(lon_rad)\n",
    "            z = ((1 - e2) * N + alt) * math.sin(lat_rad)\n",
    "            \n",
    "            return (x, y, z)\n",
    "        \n",
    "        output_schema = StructType([\n",
    "            StructField(output_cols[0], DoubleType(), True),\n",
    "            StructField(output_cols[1], DoubleType(), True),\n",
    "            StructField(output_cols[2], DoubleType(), True)\n",
    "        ])\n",
    "        \n",
    "        geodetic_to_ecef_udf = F.udf(geodetic_to_ecef, output_schema)\n",
    "        \n",
    "        dataset = dataset.withColumn(\"ecef_coords\", geodetic_to_ecef_udf(F.col(input_cols[0]), F.col(input_cols[1]), F.lit(self.defaultAltitude)))\n",
    "        \n",
    "        dataset = dataset.withColumn(output_cols[0], F.col(\"ecef_coords\")[output_cols[0]])\n",
    "        dataset = dataset.withColumn(output_cols[1], F.col(\"ecef_coords\")[output_cols[1]])\n",
    "        dataset = dataset.withColumn(output_cols[2], F.col(\"ecef_coords\")[output_cols[2]])\n",
    "        \n",
    "        dataset = dataset.drop(\"ecef_coords\")\n",
    "        \n",
    "        return dataset\n",
    "    \n",
    "    \n",
    "df = df.withColumn(\"latitude\", F.col(\"latitude\").cast(DoubleType()))\n",
    "df = df.withColumn(\"longitude\", F.col(\"longitude\").cast(DoubleType()))\n",
    "df = GeodeticToECEFTransformer(\n",
    "    inputCols=[\"latitude\", \"longitude\"],\n",
    "    outputCols=[\"x\", \"y\", \"z\"],\n",
    "    defaultAltitude=0.0\n",
    ").transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "12df641f-86cc-412b-a488-e5e77ea0e187",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "feature_cols = [\"so2\", \"co\", \"o3\", \"o3_8hr\", \"pm10\", \"pm2_5\", \"no2\", \"nox\", \"no\", \"windspeed\", \"winddirec\", \"co_8hr\", \"pm2_5_avg\", \"pm10_avg\", \"so2_avg\", \"year\", \"x\", \"y\", \"z\"]\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\", withStd=True, withMean=True)\n",
    "pipeline = Pipeline(stages=[assembler, scaler])\n",
    "model = pipeline.fit(df)\n",
    "\n",
    "df = model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3e294f6f-1347-4c11-a3fd-0610f2682276",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.select(\"scaledFeatures\", \"aqi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4e75ff00-fef6-4b55-980b-fc60fa2ca9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = df.randomSplit([0.7, 0.4], seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d501bac1-90e6-421c-9eb3-ba82eb33dc64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(scaledFeatures=DenseVector([0.2025, -0.0146, -0.4297, -1.1124, 0.05, -0.4687, -0.81, -0.3059, 0.5992, -0.9092, -0.9018, 0.1632, -0.3538, -0.1872, 0.6064, -1.1079, 0.4474, 0.7067, -0.5317]), aqi=Decimal('38.0'))]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d0bcf76a-bbf2-4b17-9a0c-a44fdaffc2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def run(command):\n",
    "    return os.popen(command).read()\n",
    "train_data.select(\"scaledFeatures\", \"aqi\")\\\n",
    "    .write\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .format(\"json\")\\\n",
    "    .save(\"project/data/train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "40d65c0b-3b35-4bef-b00f-5e58ba0da52b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run(\"hdfs dfs -cat project/data/train/*.json > data/train.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d26fe07a-ffb8-4058-a2dc-dff70c96b34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.select(\"scaledFeatures\", \"aqi\")\\\n",
    "    .write\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .format(\"json\")\\\n",
    "    .save(\"project/data/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ebba0f6a-8e5e-4733-86d7-659cc226e423",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run(\"hdfs dfs -cat project/data/test/*.json > ../data/test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50f1528a-c1d0-4eb0-b582-0fc37607efe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def run(command):\n",
    "    return os.popen(command).read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ec7da48-e98c-4ea6-96d7-41dd8dfead0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "lr = LinearRegression(featuresCol=\"scaledFeatures\", labelCol=\"aqi\")\n",
    "\n",
    "parameters = {\n",
    "    'regParam': [0.01, 0.1, 1.0],\n",
    "    'elasticNetParam': [0.0, 0.5, 1.0]\n",
    "}\n",
    "\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, parameters['regParam'])\n",
    "             .addGrid(lr.elasticNetParam, parameters['elasticNetParam'])\n",
    "             .build())\n",
    "\n",
    "crossval = CrossValidator(estimator=lr,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=RegressionEvaluator(labelCol=\"aqi\", predictionCol=\"prediction\", metricName=\"rmse\"),\n",
    "                          numFolds=3)\n",
    "\n",
    "cvModel = crossval.fit(train_data)\n",
    "bestModel = cvModel.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63d60aa5-5679-4963-824b-32705fbac99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best regParam: 0.01\n",
      "Best elasticNetParam: 0.0\n",
      "Root Mean Squared Error (RMSE) on test data = 7.014512661312377\n",
      "R2 = 0.9367780344451548\n"
     ]
    }
   ],
   "source": [
    "bestRegParam = bestModel._java_obj.getRegParam()\n",
    "bestElasticNetParam = bestModel._java_obj.getElasticNetParam()\n",
    "\n",
    "print(f\"Best regParam: {bestRegParam}\")\n",
    "print(f\"Best elasticNetParam: {bestElasticNetParam}\")\n",
    "predictions = cvModel.transform(test_data)\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol=\"aqi\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"Root Mean Squared Error (RMSE) on test data = {rmse}\")\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol=\"aqi\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "r2 = evaluator.evaluate(predictions)\n",
    "print(f\"R2 = {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "67fc98cb-80ee-4ab1-943e-089c37ae13b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestModel.write().overwrite().save(\"project/models/model1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "808d8d1f-fc34-4e78-90e4-f54f67be25f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "\n",
    "gbt = GBTRegressor(featuresCol=\"scaledFeatures\", labelCol=\"aqi\")\n",
    "\n",
    "parameters = {\n",
    "    'maxDepth': [5, 10],\n",
    "    'maxIter': [5, 10]\n",
    "}\n",
    "\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(gbt.maxDepth, parameters['maxDepth'])\n",
    "             .addGrid(gbt.maxIter, parameters['maxIter'])\n",
    "             .build())\n",
    "\n",
    "crossval = CrossValidator(estimator=gbt,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=RegressionEvaluator(labelCol=\"aqi\", predictionCol=\"prediction\", metricName=\"rmse\"),\n",
    "                          numFolds=3)\n",
    "\n",
    "cvModel = crossval.fit(train_data)\n",
    "bestModel = cvModel.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35b378d6-1b21-41bf-b804-11cee63bdcd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best maxDepth: 10\n",
      "Best maxIter: 10\n",
      "Best stepSize: 0.1\n",
      "Root Mean Squared Error (RMSE) on test data = 2.83174226002011\n",
      "R2 = 0.9900065968720342\n"
     ]
    }
   ],
   "source": [
    "bestMaxDepth = bestModel._java_obj.getMaxDepth()\n",
    "bestMaxIter = bestModel._java_obj.getMaxIter()\n",
    "bestStepSize = bestModel._java_obj.getStepSize()\n",
    "\n",
    "print(f\"Best maxDepth: {bestMaxDepth}\")\n",
    "print(f\"Best maxIter: {bestMaxIter}\")\n",
    "print(f\"Best stepSize: {bestStepSize}\")\n",
    "\n",
    "predictions = cvModel.transform(test_data)\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol=\"aqi\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"Root Mean Squared Error (RMSE) on test data = {rmse}\")\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol=\"aqi\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "r2 = evaluator.evaluate(predictions)\n",
    "print(f\"R2 = {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4ffc5b3-1883-4aa7-89eb-8f02fc1c6e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestModel.write().overwrite().save(\"project/models/model2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "71b94287-575f-40c6-ac21-3f5749e124f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 2.818808693814717\n",
      "R2 = 0.9895885309978958\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import GBTRegressionModel\n",
    "\n",
    "# Load the model\n",
    "loadedModel = GBTRegressionModel.load(\"project/models/model2\")\n",
    "\n",
    "# Assuming test_data is your DataFrame\n",
    "p = loadedModel.transform(test_data)\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol=\"aqi\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(p)\n",
    "print(f\"Root Mean Squared Error (RMSE) on test data = {rmse}\")\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol=\"aqi\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "r2 = evaluator.evaluate(p)\n",
    "print(f\"R2 = {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "be2420b9-a5ce-409d-ba22-42eebde46a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 7.081189343071962\n",
      "R2 = 0.9370092311678303\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegressionModel\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Load the model\n",
    "loadedModel = LinearRegressionModel.load(\"project/models/model1\")\n",
    "\n",
    "# Assuming test_data is your DataFrame\n",
    "p = loadedModel.transform(test_data)\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol=\"aqi\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(p)\n",
    "print(f\"Root Mean Squared Error (RMSE) on test data = {rmse}\")\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol=\"aqi\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "r2 = evaluator.evaluate(p)\n",
    "print(f\"R2 = {r2}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
