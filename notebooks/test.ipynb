{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74b02e12-1813-4a0c-9b7b-27fb27b74c31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import math\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCols, HasInputCols\n",
    "\n",
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "import os\n",
    "\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "\n",
    "\n",
    "class Encoder(Transformer, HasInputCol, HasOutputCols, DefaultParamsReadable, DefaultParamsWritable):\n",
    "    def __init__(self, inputCol, outputCols, n):\n",
    "        super(Encoder, self).__init__()\n",
    "        self._set(inputCol=inputCol, outputCols=outputCols)\n",
    "        self.n = n\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        input_col = self.getInputCol()\n",
    "        output_cols = self.getOutputCols()\n",
    "        \n",
    "        dataset = dataset.withColumn(output_cols[0], F.sin(2 * math.pi * F.col(input_col) / self.n))\n",
    "        \n",
    "        return dataset\n",
    "    \n",
    "class GeodeticToECEFTransformer(Transformer, HasInputCols, HasOutputCols, DefaultParamsReadable, DefaultParamsWritable):\n",
    "    def __init__(self, inputCols=None, outputCols=None, defaultAltitude=0.0):\n",
    "        super(GeodeticToECEFTransformer, self).__init__()\n",
    "        self._set(inputCols=inputCols, outputCols=outputCols)\n",
    "        self.defaultAltitude = defaultAltitude\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        input_cols = self.getInputCols()\n",
    "        output_cols = self.getOutputCols()\n",
    "        \n",
    "        a = 6378137.0\n",
    "        f = 1 / 298.257223563\n",
    "        b = a * (1 - f)\n",
    "        e2 = 1 - (b ** 2) / (a ** 2)\n",
    "        \n",
    "        def geodetic_to_ecef(lat, lon, alt):\n",
    "            lat_rad = math.radians(lat)\n",
    "            lon_rad = math.radians(lon)\n",
    "            \n",
    "            N = a / math.sqrt(1 - e2 * math.sin(lat_rad) ** 2)\n",
    "            \n",
    "            x = (N + alt) * math.cos(lat_rad) * math.cos(lon_rad)\n",
    "            y = (N + alt) * math.cos(lat_rad) * math.sin(lon_rad)\n",
    "            z = ((1 - e2) * N + alt) * math.sin(lat_rad)\n",
    "            \n",
    "            return (x, y, z)\n",
    "        \n",
    "        output_schema = StructType([\n",
    "            StructField(output_cols[0], DoubleType(), True),\n",
    "            StructField(output_cols[1], DoubleType(), True),\n",
    "            StructField(output_cols[2], DoubleType(), True)\n",
    "        ])\n",
    "        \n",
    "        geodetic_to_ecef_udf = F.udf(geodetic_to_ecef, output_schema)\n",
    "        \n",
    "        dataset = dataset.withColumn(\"ecef_coords\", geodetic_to_ecef_udf(F.col(input_cols[0]), F.col(input_cols[1]), F.lit(self.defaultAltitude)))\n",
    "        \n",
    "        dataset = dataset.withColumn(output_cols[0], F.col(\"ecef_coords\")[output_cols[0]])\n",
    "        dataset = dataset.withColumn(output_cols[1], F.col(\"ecef_coords\")[output_cols[1]])\n",
    "        dataset = dataset.withColumn(output_cols[2], F.col(\"ecef_coords\")[output_cols[2]])\n",
    "        \n",
    "        dataset = dataset.drop(\"ecef_coords\")\n",
    "        \n",
    "        return dataset\n",
    "    \n",
    "def run(command):\n",
    "    return os.popen(command).read()\n",
    "\n",
    "team = 39\n",
    "\n",
    "warehouse = \"project/hive/warehouse\"\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "        .appName(\"{} - spark ML\".format(team))\\\n",
    "        .master(\"yarn\")\\\n",
    "        .config(\"hive.metastore.uris\", \"thrift://hadoop-02.uni.innopolis.ru:9883\")\\\n",
    "        .config(\"spark.sql.warehouse.dir\", warehouse)\\\n",
    "        .config(\"spark.sql.avro.compression.codec\", \"snappy\")\\\n",
    "        .enableHiveSupport()\\\n",
    "        .getOrCreate()\n",
    "        \n",
    "metrics = spark.read.format(\"avro\").table('team39_projectdb.metrics_part')\n",
    "stations = spark.read.format(\"avro\").table('team39_projectdb.stations')\n",
    "\n",
    "df = metrics.join(stations, on='sid', how='inner')\n",
    "\n",
    "df = df.withColumn(\"year\", F.year(\"date_time\"))\n",
    "df = df.withColumn(\"month\", F.month(\"date_time\"))\n",
    "df = df.withColumn(\"day\", F.dayofmonth(\"date_time\"))\n",
    "df = df.withColumn(\"hour\", F.hour(\"date_time\"))\n",
    "df = Encoder(\"month\", [\"month\"], 12).transform(df)\n",
    "df = Encoder(\"day\", [\"day\"], 31).transform(df)\n",
    "df = Encoder(\"hour\", [\"hour\"], 24).transform(df)\n",
    "\n",
    "df = df.withColumn(\"latitude\", F.col(\"latitude\").cast(DoubleType()))\n",
    "df = df.withColumn(\"longitude\", F.col(\"longitude\").cast(DoubleType()))\n",
    "\n",
    "df = GeodeticToECEFTransformer(\n",
    "    inputCols=[\"latitude\", \"longitude\"],\n",
    "    outputCols=[\"x\", \"y\", \"z\"],\n",
    "    defaultAltitude=0.0\n",
    ").transform(df)\n",
    "\n",
    "feature_cols = [\"so2\", \"co\", \"o3\", \"o3_8hr\", \"pm10\", \"pm2_5\", \"no2\", \"nox\", \"no\", \"windspeed\", \"winddirec\", \"co_8hr\", \"pm2_5_avg\", \"pm10_avg\", \"so2_avg\", \"year\", \"x\", \"y\", \"z\"]\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\", withStd=True, withMean=True)\n",
    "pipeline = Pipeline(stages=[assembler, scaler])\n",
    "model = pipeline.fit(df)\n",
    "\n",
    "df = model.transform(df)\n",
    "\n",
    "df = df.select(\"scaledFeatures\", \"aqi\")\n",
    "\n",
    "train_data, test_data = df.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "train_data.select(\"scaledFeatures\", \"aqi\")\\\n",
    "    .write\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .format(\"json\")\\\n",
    "    .save(\"project/data/train\")\n",
    "    \n",
    "run(\"hdfs dfs -cat project/data/train/*.json > ../data/train.json\")\n",
    "\n",
    "test_data.select(\"scaledFeatures\", \"aqi\")\\\n",
    "    .write\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .format(\"json\")\\\n",
    "    .save(\"project/data/test\")\n",
    "    \n",
    "run(\"hdfs dfs -cat project/data/test/*.json > ../data/test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90990eec-5411-418d-ab77-32e8fa2e5da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(featuresCol=\"scaledFeatures\", labelCol=\"aqi\")\n",
    "\n",
    "parameters = {\n",
    "    'regParam': [0.01, 0.1],\n",
    "    'elasticNetParam': [0.0, 0.5]\n",
    "}\n",
    "\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, parameters['regParam'])\n",
    "             .addGrid(lr.elasticNetParam, parameters['elasticNetParam'])\n",
    "             .build())\n",
    "\n",
    "crossval = CrossValidator(estimator=lr,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=RegressionEvaluator(labelCol=\"aqi\", predictionCol=\"prediction\", metricName=\"rmse\"),\n",
    "                          numFolds=3)\n",
    "\n",
    "cvModel = crossval.fit(train_data)\n",
    "bestModel = cvModel.bestModel\n",
    "bestRegParam = bestModel._java_obj.getRegParam()\n",
    "bestElasticNetParam = bestModel._java_obj.getElasticNetParam()\n",
    "\n",
    "\n",
    "\n",
    "bestModel.write().overwrite().save(\"project/models/model1\")\n",
    "run(\"hdfs dfs -get project/models/model1 ../models/model1\")\n",
    "\n",
    "\n",
    "p = bestModel.transform(test_data)\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol=\"aqi\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse_1 = evaluator.evaluate(p)\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol=\"aqi\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "r2_1 = evaluator.evaluate(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f87dcbc-444a-4095-823e-428a8a4f8363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(scaledFeatures=DenseVector([-0.3306, -0.0552, 0.2004, -0.2834, 0.0967, -0.1268, 0.4068, 0.2053, -0.2072, 0.224, -1.1521, 0.7571, 0.4777, 0.6414, 0.6064, -1.1076, -0.0622, -0.8021, 1.0473]), aqi=Decimal('63.0'), prediction=62.0496806076676)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1b4e959-08a6-4906-8de1-5d5ab5c6e4eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = p.drop(\"scaledFeatures\")\n",
    "\n",
    "\n",
    "# Save the DataFrame to CSV\n",
    "p.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"sep\", \",\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .save(\"project/output/model1_predictions\")\n",
    "\n",
    "# Optionally, concatenate the CSV files into a single file\n",
    "run(\"hdfs dfs -cat project/output/model1_predictions/*.csv > ../output/model1_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a36f93a-6620-450b-9430-439d7ef0e5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt = GBTRegressor(featuresCol=\"scaledFeatures\", labelCol=\"aqi\")\n",
    "\n",
    "parameters = {\n",
    "    'maxDepth': [3, 10],\n",
    "    'maxIter': [3, 10]\n",
    "}\n",
    "\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(gbt.maxDepth, parameters['maxDepth'])\n",
    "             .addGrid(gbt.maxIter, parameters['maxIter'])\n",
    "             .build())\n",
    "\n",
    "crossval = CrossValidator(estimator=gbt,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=RegressionEvaluator(labelCol=\"aqi\", predictionCol=\"prediction\", metricName=\"rmse\"),\n",
    "                          numFolds=3)\n",
    "\n",
    "cvModel = crossval.fit(train_data)\n",
    "bestModel2 = cvModel.bestModel\n",
    "\n",
    "bestMaxDepth = bestModel2._java_obj.getMaxDepth()\n",
    "bestMaxIter = bestModel2._java_obj.getMaxIter()\n",
    "bestStepSize = bestModel2._java_obj.getStepSize()\n",
    "\n",
    "bestModel2.write().overwrite().save(\"project/models/model2\")\n",
    "run(\"hdfs dfs -get project/models/model2 ../models/model3\")\n",
    "\n",
    "\n",
    "p = bestModel2.transform(test_data)\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol=\"aqi\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse_2 = evaluator.evaluate(p)\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol=\"aqi\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "r2_2 = evaluator.evaluate(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb9ae726-7bd0-4605-8bac-e87e739447b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = p.drop(\"scaledFeatures\")\n",
    "\n",
    "\n",
    "p.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"sep\", \",\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .save(\"project/output/model2_predictions\")\n",
    "\n",
    "# Optionally, concatenate the CSV files into a single file\n",
    "run(\"hdfs dfs -cat project/output/model2_predictions/*.csv > ../output/model2_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb856a7c-da14-445f-9b89-40049fa2ff0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------+------------------+------------------+\n",
      "|model                                                                         |RMSE              |R2                |\n",
      "+------------------------------------------------------------------------------+------------------+------------------+\n",
      "|LinearRegressionModel: uid=LinearRegression_9735c20c947f, numFeatures=19      |6.992850470303261 |0.9375176466985905|\n",
      "|GBTRegressionModel: uid=GBTRegressor_22bd8921cc58, numTrees=10, numFeatures=19|2.8846551180605844|0.9895039584925475|\n",
      "+------------------------------------------------------------------------------+------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = [[str(bestModel),rmse_1, r2_1], [str(bestModel2),rmse_2, r2_2]]\n",
    "\n",
    "df = spark.createDataFrame(models, [\"model\", \"RMSE\", \"R2\"])\n",
    "df.show(truncate=False)\n",
    "\n",
    "df.coalesce(1)\\\n",
    "    .write\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .format(\"csv\")\\\n",
    "    .option(\"sep\", \",\")\\\n",
    "    .option(\"header\",\"true\")\\\n",
    "    .save(\"project/output/evaluation.csv\")\n",
    "\n",
    "# Run it from root directory of the repository\n",
    "run(\"hdfs dfs -cat project/output/evaluation.csv/*.csv > ../output/evaluation.csv\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
